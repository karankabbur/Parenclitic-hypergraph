{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff7686-c7f3-4de8-be4f-52bc1284290b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from math import comb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import operator\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import os\n",
    "from scipy.io import savemat, loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a0ea7-35cd-4ed4-80c6-48055d7dbdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081dc6a-d22b-45cd-a455-d889a33fc977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_coupling_case1(alpha):\n",
    "    # Input:\n",
    "    # alpha: vector of random numbers unif(0,1)\n",
    "\n",
    "    # Output:\n",
    "    # f: vector of dim(10), features f1, f2, ..., f10\n",
    "    import numpy as np\n",
    "    f = np.zeros(10)\n",
    "    \n",
    "    # linear correlation btw f1, f2, f3\n",
    "    # triadic\n",
    "    # 2*f1 + f2 = 3*f3\n",
    "    f[0] = alpha[0]\n",
    "    f[1]=alpha[1]\n",
    "    f[2]=(2*alpha[0]+alpha[1])/3 \n",
    "    \n",
    "    # linear correlation btw f4, f5, f6\n",
    "    # triadic\n",
    "    # f4 + f5 = 4*f6\n",
    "    f[3]=(3*alpha[2]+2*alpha[3])/5 \n",
    "    f[4]=(alpha[2]+2*alpha[3])/3 \n",
    "    f[5]=(2*alpha[2]+alpha[3])/3 \n",
    "    \n",
    "    # linear correlation btw f7, f8, f9\n",
    "    # triadic\n",
    "    # f8 + f9 = 2*f7\n",
    "    f[6]=(alpha[4]+alpha[5])/2\n",
    "    f[7]=(alpha[4]+3*alpha[5])/4 \n",
    "    f[8]=(3*alpha[4]+alpha[5])/4 \n",
    "\n",
    "    # f10, singleton\n",
    "    f[9]=alpha[6]\n",
    "\n",
    "    return f\n",
    "\n",
    "def features_coupling_case2(alpha):\n",
    "    # Input:\n",
    "    # alpha: vector of random numbers unif(0,1)\n",
    "\n",
    "    # Output:\n",
    "    # f: vector of dim(10), features f1, f2, ..., f10\n",
    "    import numpy as np\n",
    "    f = np.zeros(10)\n",
    "    \n",
    "    f[0] = alpha[1]\n",
    "    f[1]=alpha[1]**2\n",
    "    f[2]=alpha[1]**4 \n",
    "    \n",
    "    # linear correlation btw f4, f5, f6\n",
    "    # triadic\n",
    "    # f4 + f5 = 4*f6\n",
    "    f[3]=(3*alpha[2]+2*alpha[3])/5 \n",
    "    f[4]=(alpha[2]+2*alpha[3])/3 \n",
    "    f[5]=(2*alpha[2]+alpha[3])/3 \n",
    "    \n",
    "    f[6]=(alpha[4]+alpha[5])/2\n",
    "    f[7]=(alpha[4]+3*alpha[5])/4 \n",
    "    f[8]=(3*alpha[4]+alpha[5])/4 \n",
    "\n",
    "    # f10, singleton\n",
    "    f[9]=alpha[6]\n",
    "\n",
    "    return f\n",
    "\n",
    "def features_coupling_case3(alpha):\n",
    "    # Input:\n",
    "    # alpha: vector of random numbers unif(0,1)\n",
    "\n",
    "    # Output:\n",
    "    # f: vector of dim(10), features f1, f2, ..., f10\n",
    "    import numpy as np\n",
    "    f = np.zeros(10)\n",
    "    \n",
    "    f[0] = alpha[1]\n",
    "    f[1]=alpha[1]**2\n",
    "    f[2]=alpha[1]**4 \n",
    "    \n",
    "    # linear correlation btw f4, f5, f6\n",
    "    # triadic\n",
    "    # f4 + f5 = 4*f6\n",
    "    f[3]=(3*alpha[2]+2*alpha[3])/5 \n",
    "    f[4]=(alpha[2]+2*alpha[3])/3 \n",
    "    f[5]=(2*alpha[2]+alpha[3])/3 \n",
    "    \n",
    "    f[6]=(alpha[4]**2 + alpha[5]**4)/2\n",
    "    f[7]=( np.sin(alpha[4]) + alpha[5])/(np.sin(1)+1) \n",
    "    f[8]=(alpha[4]**3 + np.tan(alpha[5]) )/(1+np.tan(1)) \n",
    "\n",
    "    # f10, singleton\n",
    "    f[9]=alpha[6]\n",
    "\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f4e84e-06bc-42d8-8b40-88df095b66b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ ALTERNATIVE WAY. #############################\n",
    "##### Uses Matrix operations, faster than previous code.  ####################\n",
    "\n",
    "class generate_synthetic_data:\n",
    "    def __init__(self, n_features, N, coupling_func, *random_seed):\n",
    "        self.n_features = n_features\n",
    "        self.N = N\n",
    "        self.coupling_func = coupling_func\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "    def data_matrix(self):\n",
    "        # Inputs:\n",
    "        # n_features: number of features for an instance\n",
    "        # N: number of instances\n",
    "    \n",
    "        # Output:\n",
    "        # f_mat: (n_features x N) dim matrix of artificial data\n",
    "        # obtained from pre-defined coupling among the features.\n",
    "        # f_mat = feature matrix\n",
    "        \n",
    "        # Initialize the feature matrix\n",
    "        f_mat = np.zeros((self.n_features, self.N))\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            alpha = np.random.uniform(size=self.n_features)\n",
    "            \n",
    "            # assign values to each column\n",
    "            f_vector = self.coupling_func(alpha)\n",
    "    \n",
    "            # Add a check\n",
    "            if len(f_vector)!=self.n_features:\n",
    "                print(\"Error: No. of synthetic coupled features do not match with n_features!!\")\n",
    "                break\n",
    "            else:\n",
    "                f_mat[:, i] = f_vector\n",
    "            \n",
    "        return f_mat\n",
    "\n",
    "\n",
    "class disentangle_parenclitic_hypergraph:\n",
    "    \n",
    "    def __init__(self, f_mat, M, random_seed, dist_type, \\\n",
    "                 theta, uniform_size, frac=0.0, noise=(0.0,0.0), p=10):\n",
    "        # Input:\n",
    "        # f_mat: data matrix (num features x num instances), (n_features x N)\n",
    "        # M: No. of random points to be introduced\n",
    "        # random_seed for reproducibility of added noise\n",
    "        \n",
    "        # dist_type: for our analysis \"min\" is to be set\n",
    "        # but other distrances like mean, max, percentile-median can also be given as input\n",
    "\n",
    "        # l: no. of features set (f_i, f_j, f_k) to select which has \n",
    "        \n",
    "        # noise is tuple (mean, std) for gaussian sampling\n",
    "        self.n_features = f_mat.shape[0]\n",
    "        self.N = f_mat.shape[1]\n",
    "        self.f_mat = f_mat\n",
    "        \n",
    "        self.M = M\n",
    "        \n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        self.dist_type = dist_type\n",
    "        if self.dist_type==\"percentile_median\":\n",
    "            self.p = p\n",
    "            # else ignore the parameter 'p'\n",
    "        \n",
    "        self.theta = theta\n",
    "        self.uniform_size = uniform_size\n",
    "\n",
    "        #choosing fraction of (fN) elements of synthetic data\n",
    "        self.frac = frac\n",
    "\n",
    "        # Initializes the dictionary whose \"keys\" being all the possible hyperedges\n",
    "        # \"values\" being count of number of times a feature hyperedge(i,j,k)\n",
    "        # has a random number (identity m) with maximum deviation from N points.\n",
    "        #self.L = {}\n",
    "        #for h in self.k_uniform_hyperedges():\n",
    "        #    self.L[h]=0                   \n",
    "        #self.all_hyperedges = list(self.k_uniform_hyperedges())\n",
    "        self.n_comb = math.comb(n_features, uniform_size)# 120\n",
    "        self.L = {h: 0 for h in self.k_uniform_hyperedges()}\n",
    "        \n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "        if self.frac!=0.0:\n",
    "            self.noise_mean = noise[0]\n",
    "            self.noise_std = noise[1]\n",
    "        \n",
    "            self.add_noise()\n",
    "\n",
    "    def add_noise(self):\n",
    "        f_ind = np.random.randint(self.n_features, size=int(self.frac*self.n_features*self.N))\n",
    "        N_ind = np.random.randint(self.N, size=int(self.frac*self.n_features*self.N))\n",
    "\n",
    "        #ind_list = list(zip(f_ind, N_ind))\n",
    "        noise_list = np.random.normal(self.noise_mean, self.noise_std, size=len(f_ind))\n",
    "        for i in range(len(f_ind)):\n",
    "            self.f_mat[f_ind[i], N_ind[i]] += noise_list[i]\n",
    "        return None    \n",
    "    \n",
    "    def k_uniform_hyperedges(self):\n",
    "        # This function is used to generalize to k-uniform hypergraph\n",
    "        # where k is not just equal to 3 referring to triadic interaction\n",
    "        \n",
    "        # Inputs:\n",
    "        # n_features: no. of features of the data\n",
    "        # uniform_size: size of hyperedges of k-uniform hypergraph\n",
    "    \n",
    "        # Outputs:\n",
    "        # list of all k-uniform hyperedges\n",
    "    \n",
    "        return combinations(range(self.n_features), self.uniform_size)\n",
    "    \n",
    "    \n",
    "    def inter_cluster_distance(self, distance_list):\n",
    "        # Computes the inter-cluster distances between two clusters\n",
    "        # singleton cluster which is a random number with n_features \"m_random\"\n",
    "        \n",
    "        # Single linkage\n",
    "        if self.dist_type == \"min\":\n",
    "            return np.min(distance_list, axis=1)\n",
    "    \n",
    "        # Complete linkage\n",
    "        elif self.dist_type == \"max\":\n",
    "            return np.max(distance_list, axis=1)\n",
    "    \n",
    "        # Average linkage\n",
    "        elif self.dist_type == \"mean\":\n",
    "            return np.mean(distance_list, axis=1)\n",
    "\n",
    "        # Sorted dist Percentile median\n",
    "        # robust against noise and outliers\n",
    "        elif self.dist_type == \"percentile_median\":\n",
    "            d_p = np.percentile(np.sort(distance_list, axis=1), self.p)\n",
    "            D_p = distance_list[:, distance_list<=d_p]\n",
    "            return np.median(D_p, axis=1)\n",
    "    \n",
    "        else:\n",
    "            print(\" Error: type = min/max/mean/percentile_median \")\n",
    "\n",
    "    \n",
    "    def compute_pairwise_distances(self, h, random_matrix):\n",
    "        # computing pairwise distances between N points and 1 random point \n",
    "        # in (fi, fj, fk, ...) k-projected feature space.\n",
    "    \n",
    "        # Inputs:\n",
    "        # h: hyperedge with its elements being indices (f1, f2, f3, ...)\n",
    "        # f_mat: synthetic data with higher order interaction embedded into it\n",
    "        # m_random: uniform random number belonging to R^(n_features)\n",
    "    \n",
    "        # Output:\n",
    "        # dist: vector of dim(N). pairwise distance between N individuals and\n",
    "        # random number m in the k-projected feature space (f1, f2, f3, ...) \n",
    "    \n",
    "        # ssd -> sum of squared difference\n",
    "        #N=f_mat.shape[1]\n",
    "        #ssd = np.zeros(N)\n",
    "        #for ind in h:\n",
    "        #    ssd += (f_mat[ind, :] - m_random[ind])**2\n",
    "        #dist=np.sqrt(ssd)\n",
    "        # M x N matrix\n",
    "        \n",
    "        dist = cdist(random_matrix[:, np.array(h)], self.f_mat[np.array(h),:].T)\n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def freq_maxDev_h(self):\n",
    "        random_matrix = np.random.uniform(size=(self.M, self.n_features))\n",
    "\n",
    "        L_matrix = np.zeros((self.M, self.n_comb))\n",
    "\n",
    "        for i, h in enumerate(self.k_uniform_hyperedges()):\n",
    "            MxN_dist_matrix = self.compute_pairwise_distances(h, random_matrix)\n",
    "            L_matrix[:, i] = self.inter_cluster_distance(MxN_dist_matrix)\n",
    "        \n",
    "        #print(L_matrix.shape)\n",
    "        for i in range(self.M):\n",
    "            # indices of top 'l' hyperedges of range(n_comb)\n",
    "            ind_list = np.argwhere(L_matrix[i,:]>self.theta).flatten()\n",
    "            #ind_list = np.argsort(L_matrix[i, :])[-self.l:]\n",
    "            for j in ind_list:\n",
    "                h_paren = list(self.k_uniform_hyperedges())[j]\n",
    "                self.L[h_paren]+=1\n",
    "\n",
    "        return self.L\n",
    "\n",
    "\n",
    "    def overlap_coefficient(self):\n",
    "        L = self.freq_maxDev_h()\n",
    "        \n",
    "        cluster_B_keys = [(0,1,2), (3,4,5), (6,7,8)]\n",
    "        cluster_A_keys = set(list(L.keys())) - set(cluster_B_keys)\n",
    "        \n",
    "        cluster_B = [L[key] for key in cluster_B_keys]\n",
    "        cluster_A = [L[key] for key in cluster_A_keys]\n",
    "        \n",
    "        cluster_A, cluster_B = np.array(cluster_A), np.array(cluster_B)\n",
    "        \n",
    "        # Assuming cluster_A and cluster_B are the two clusters with continuous values in 1D\n",
    "    \n",
    "        # Calculate the intersection of the two clusters\n",
    "        intersection = max(0, min(cluster_A.max(), cluster_B.max()) - max(cluster_A.min(), cluster_B.min()))\n",
    "        \n",
    "        # Calculate the minimum range spanned by the clusters\n",
    "        min_range = min(cluster_A.max() - cluster_A.min(), cluster_B.max() - cluster_B.min())\n",
    "        # Compute the overlap coefficient\n",
    "        if min_range == 0:\n",
    "            overlap_coefficient = 1.0\n",
    "        else:\n",
    "            overlap_coefficient = intersection / min_range\n",
    "        return overlap_coefficient\n",
    "\n",
    "    def cluster_ranges(self):\n",
    "        L = self.freq_maxDev_h()\n",
    "\n",
    "        cluster_B_keys = [(0,1,2), (3,4,5), (6,7,8)]\n",
    "        cluster_A_keys = set(list(L.keys())) - set(cluster_B_keys)\n",
    "        \n",
    "        cluster_B = [L[key] for key in cluster_B_keys]\n",
    "        cluster_A = [L[key] for key in cluster_A_keys]\n",
    "        \n",
    "        cluster_A, cluster_B = np.array(cluster_A), np.array(cluster_B)\n",
    "        \n",
    "        return np.array([min(cluster_A), max(cluster_A), min(cluster_B), max(cluster_B)])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e882b5da-bf46-4bb7-8afe-1117a221b919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"./data/\"\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "image_path = \"./image/\"\n",
    "if not os.path.isdir(image_path):\n",
    "    os.mkdir(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a2c3f9-c41f-4a92-bdf3-3d06cf4f2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b86447-7e01-4fe2-a3ee-1bc23b01e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise\n",
    "n_features, N, M = 10, 10**4, 10**4\n",
    "dist_type = \"min\"\n",
    "\n",
    "uniform_size = 3\n",
    "realizations = 100\n",
    "n_comb = math.comb(n_features, uniform_size)\n",
    "theta_vals = np.arange(0.05, 0.8+0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1e4dc-6abb-4c0b-8df8-750080b80e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_freq_hedges(args, f_mat, M, dist_type,\\\n",
    "                    uniform_size, frac, noise):\n",
    "    random_seed2, theta = args[0], args[1]    \n",
    "    class_obj = disentangle_parenclitic_hypergraph(f_mat, M, random_seed2, dist_type, theta, \\\n",
    "                                           uniform_size, frac, noise)\n",
    "    L = class_obj.freq_maxDev_h()\n",
    " \n",
    "    return list(L.values()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720842e-4ed5-400d-9d93-6a5ee9ef41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for perturbation in [\"without_noise\", \"with_noise\"]:\n",
    "    print(perturbation)\n",
    "    if perturbation==\"without_noise\":\n",
    "        frac = 0.0\n",
    "        noise = (0.0,0.0)\n",
    "    else:\n",
    "        frac = 0.1\n",
    "        noise = (0.0,0.05)\n",
    "\n",
    "    vary_theta_dict = {}\n",
    "    for case_num in [1,2,3]:\n",
    "        \n",
    "        print(\"case num:\",case_num)\n",
    "        if case_num==1:\n",
    "            coupling_func = features_coupling_case1\n",
    "        elif case_num==2:\n",
    "            coupling_func = features_coupling_case2\n",
    "        elif case_num==3:\n",
    "            coupling_func = features_coupling_case3\n",
    "            \n",
    "        L_matrix = np.zeros((realizations, len(theta_vals), n_comb))\n",
    "\n",
    "        i=0\n",
    "        # random_seed1 is for generating same data \n",
    "        for random_seed1 in range(123, 123+realizations):\n",
    "\n",
    "            f_mat = generate_synthetic_data(n_features, N, coupling_func, random_seed1).data_matrix()\n",
    "\n",
    "            # random_seed2 is for generating same random noise\n",
    "            opt_model = partial( count_freq_hedges, f_mat=f_mat, M=M, dist_type=dist_type, \\\n",
    "                            uniform_size=uniform_size, frac=frac, noise=noise)\n",
    "\n",
    "            pool = multiprocess.Pool( 16 )  \n",
    "            rand_seeds = range( i+1 + i*L_matrix.shape[1], i+1+L_matrix.shape[1]+ i*L_matrix.shape[1])\n",
    "\n",
    "            L_matrix[i,:, :] = pool.map(opt_model, zip(rand_seeds, theta_vals) )\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "            vary_theta_dict[\"case_%d\"%case_num] = L_matrix\n",
    "\n",
    "            i+=1\n",
    "            if (i+1)%10==0:\n",
    "                print(i+1)\n",
    "    \n",
    "    savemat(data_path+\"%s.mat\"%perturbation, vary_theta_dict)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5fb5b-44bb-43af-9c99-44a0056799bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940ee65-5dca-450f-9509-31b0aa7bdfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618a014-f339-4392-8771-83aa2ba730e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f6f88-ec3d-4103-83dc-9d8e94c6d0a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class compute_properties_l:\n",
    "    \n",
    "    def __init__(self, arr, n_features, uniform_size):\n",
    "        \n",
    "        # arr: (realization, l, all triplet feature combinations)\n",
    "        \n",
    "        \n",
    "        self.arr = arr\n",
    "        self.n_features = n_features\n",
    "        self.uniform_size=uniform_size\n",
    "        \n",
    "        self.h_edges = list(combinations(range(self.n_features),\\\n",
    "                                    self.uniform_size))\n",
    "        self.h_corr = [(0,1,2), (3,4,5), (6,7,8)]\n",
    "\n",
    "        self.cluster_A_bool = [i not in self.h_corr for i in self.h_edges ]\n",
    "        self.cluster_B_bool = [i in self.h_corr for i in self.h_edges ]\n",
    "        \n",
    "        self.arr_norm = np.zeros((self.arr.shape[0], \\\n",
    "                                  self.arr.shape[1],\\\n",
    "                                  self.arr.shape[2]))\n",
    "        for i in range(self.arr.shape[0]):\n",
    "            scaler=MinMaxScaler()\n",
    "            self.arr_norm[i,:,:] = scaler.fit_transform(self.arr[i,:,:].T).T\n",
    "        \n",
    "    def compute_cluster_prop(self, cluster_name, prop_name):\n",
    "        if cluster_name==\"A\":\n",
    "            data = self.arr_norm[:, :, self.cluster_A_bool]\n",
    "        elif cluster_name==\"B\":\n",
    "            data = self.arr_norm[:, :, self.cluster_B_bool]\n",
    "        else:\n",
    "            print(\"Currently there are only 2 clusters 'A', 'B'.\")\n",
    "\n",
    "        mean_values = []\n",
    "        std_values = []\n",
    "        # looping over each l value, averaging over each realization\n",
    "        for i in range(self.arr_norm.shape[1]):\n",
    "\n",
    "            if prop_name=='min':\n",
    "                min_values = np.min(data[:,i,:], axis=1)\n",
    "                mean_values.append(np.mean(min_values))\n",
    "                std_values.append(np.std(min_values))\n",
    "\n",
    "            elif prop_name=='max':\n",
    "                max_values = np.max(data[:,i,:], axis=1)\n",
    "                mean_values.append(np.mean(max_values))\n",
    "                std_values.append(np.std(max_values))\n",
    "            \n",
    "            elif prop_name=='centroid':\n",
    "                mu_values = np.mean(data[:,i,:], axis=1)\n",
    "                mean_values.append(np.mean(mu_values))\n",
    "                std_values.append(np.std(mu_values))\n",
    "\n",
    "            elif prop_name=='width':\n",
    "                width_vaues = np.max(data[:,i,:], axis=1)-np.min(data[:,i,:], axis=1)\n",
    "                mean_values.append(np.mean(width_vaues))\n",
    "                std_values.append(np.std(width_vaues))\n",
    "\n",
    "        return np.array(mean_values), np.array(std_values)\n",
    "\n",
    "    def compute_intercluster_dist(self):\n",
    "        data_A = self.arr_norm[:, :, self.cluster_A_bool]\n",
    "        data_B = self.arr_norm[:, :, self.cluster_B_bool]\n",
    "\n",
    "        mean_values = []\n",
    "        std_values = []\n",
    "\n",
    "        n_realizations = self.arr.shape[0]\n",
    "        \n",
    "        # looping over each l value, averaging over each realization\n",
    "        for i in range(self.arr.shape[1]):\n",
    "            dist_l = []\n",
    "            for j in range(n_realizations):\n",
    "                pairwise_dist=[]\n",
    "                for p1 in data_A[j,i,:]:\n",
    "                    for p2 in data_B[j,i,:]:\n",
    "                        pairwise_dist.append(np.abs(p2-p1))\n",
    "                dist_l.append(np.min(pairwise_dist))\n",
    "                \n",
    "            mean_values.append(np.mean(dist_l))\n",
    "            std_values.append(np.std(dist_l))\n",
    "        \n",
    "        return np.array(mean_values), np.array(std_values)\n",
    "\n",
    "    def overlap_coefficient(self):\n",
    "        \n",
    "        data_A = self.arr[:, :, self.cluster_A_bool]\n",
    "        data_B = self.arr[:, :, self.cluster_B_bool]\n",
    "\n",
    "        mean_values = []\n",
    "        std_values = []\n",
    "        n_realizations = self.arr.shape[0]\n",
    "        \n",
    "        # looping over each l value, averaging over each realization\n",
    "        for i in range(self.arr.shape[1]):\n",
    "            overlap_l = []\n",
    "            for j in range(n_realizations):\n",
    "                \n",
    "                min_A, max_A, min_B, max_B = min(data_A[j,i,:]), max(data_A[j,i,:]),\\\n",
    "                                             min(data_B[j,i,:]), max(data_B[j,i,:])   \n",
    "                # Calculate the intersection of the two clusters\n",
    "                intersection = max(0, min(max_A, max_B) - max(min_A, min_B))\n",
    "\n",
    "                # Calculate the minimum range spanned by the clusters\n",
    "                min_range = min(max_A - min_A, max_B - min_B)\n",
    "                # Compute the overlap coefficient\n",
    "                if min_range == 0:\n",
    "                    overlap_coefficient = 1.0\n",
    "                else:\n",
    "                    overlap_coefficient = intersection / min_range\n",
    "                overlap_l.append(overlap_coefficient)\n",
    "            mean_values.append(np.mean(overlap_l))\n",
    "            std_values.append(np.std(overlap_l))\n",
    "        return np.array(mean_values), np.array(std_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807336ac-f785-4946-9b2a-82ad9e4951c1",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d140cd1-c6c0-4928-8a02-094eee1eb4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# Get the 'tab20' colormap\n",
    "cmap = mpl.colormaps.get_cmap('tab20')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231b78e-f6f1-418c-a074-e2679681f5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# intercluster distance\n",
    "for case_num in [1,2,3]:\n",
    "\n",
    "    fig,ax= plt.subplots(figsize=(5,3))\n",
    "\n",
    "    for perturbation in ['without_noise', 'with_noise']:\n",
    "\n",
    "        arr = loadmat(data_path+\"%s.mat\"%perturbation)[\"case_%d\"%case_num]\n",
    "        comp = compute_properties_l(arr, n_features, uniform_size) \n",
    "\n",
    "        if perturbation==\"without_noise\":\n",
    "            color = cmap(0)\n",
    "        else:\n",
    "            color = cmap(2)\n",
    "\n",
    "        mean, std = comp.overlap_coefficient()\n",
    "\n",
    "        ax.plot(theta_vals, mean, color=color, label=perturbation)\n",
    "        ax.fill_between(theta_vals, mean-std,mean+std, alpha=0.3, color='grey')\n",
    "\n",
    "    ax.set_ylabel(\"Overlap coef.\", size=13)\n",
    "    ax.set_xlabel(r\"$\\theta$\", size=13)\n",
    "    #ax.set_title(\"Case %s\"%case_num, size=13)\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    plt.savefig(image_path+\"case%s_overlap_coef.pdf\"%case_num,  facecolor=\"white\", \\\n",
    "    bbox_inches=\"tight\", dpi=600 )\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36dfd2-2afe-4b6c-b189-929eedd9e76f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973b0de-beff-497a-b970-25e94bf8275c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78472218-b8dd-4de1-9d34-795bb589aaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e46708d-37ca-4eb5-864f-1432c8b1a1ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
